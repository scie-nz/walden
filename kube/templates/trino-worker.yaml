# The trino worker, with colocated alluxio worker container.
# Ideally, these should be given as much RAM as possible.
# The alluxio workers must be accessible over localhost by clients.
# Another strategy would be to run the alluxio workers as a daemonset,
# but in this case they are only being accessed via the trino workers.
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: trino-worker
  name: trino-worker
  namespace: walden
spec:
  serviceName: trino-worker
  # Allow changes to deploy in parallel
  podManagementPolicy: Parallel
  replicas: {{ .Values.trino.worker_replicas }}
  selector:
    matchLabels:
      app: trino-worker
  template:
    metadata:
      labels:
        app: trino-worker
    spec:
      nodeSelector:
        kubernetes.io/arch: amd64
{{- if .Values.node.selector.trino_worker }}
{{ toYaml .Values.node.selector.trino_worker | indent 8 }}
{{- end }}
{{- if .Values.node.toleration.trino_worker }}
      tolerations:
{{ toYaml .Values.node.toleration.trino_worker | indent 8 }}
{{- end }}
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      terminationGracePeriodSeconds: 10
{{- if .Values.alluxio.enabled }}
      initContainers:
      - name: wait-for-alluxio
        image: {{ .Values.image.busybox }}
        command:
        - /bin/sh
        - -c
        - until nc -zv $ALLUXIO_LEADER_HOST $ALLUXIO_LEADER_PORT -w1; do echo 'waiting for alluxio-leader'; sleep 1; done
        env:
        - name: ALLUXIO_LEADER_HOST
          value: alluxio
        - name: ALLUXIO_LEADER_PORT
          value: "19999"
{{- end }}

      containers:

      - name: trino-worker
        image: {{ .Values.image.trino }}
        command:
        - /bin/bash
        - -c
        # 1. Create diskcache/memcache directories for hive and any other catalogs.
        # 2. Copy read-only configmap files into /trino-server/etc[/catalog]
        # 3. trino-launch.sh: Render any templated configs, then start trino
        - |
          mkdir -p /diskcache/hive /memcache/hive &&
{{- range $name, $props := .Values.trino.custom_catalogs }}
          mkdir -p /diskcache/{{ $name }} /memcache/{{ $name }} &&
{{- end }}
          mkdir -p /trino-server/etc/catalog &&
          cp -v /tmp/roconf/* /trino-server/etc &&
          cp -v /tmp/rocatalog/* /trino-server/etc/catalog &&
          /bin/bash /trino-server/etc/trino-launch.sh
        env:
        - name: HIVE_METASTORE_HOST
          value: metastore
        - name: HIVE_METASTORE_PORT
          value: "9083"
        - name: MINIO_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: minio-root
              key: user
        - name: MINIO_ACCESS_KEY_SECRET
          valueFrom:
            secretKeyRef:
              name: minio-root
              key: pass
        - name: CONFIG_JVM_HEAP
          value: "{{ .Values.trino.mem_jvm_heap }}"
        - name: CONFIG_QUERY_MAX_MEMORY_PER_NODE
          value: "{{ .Values.trino.config.query_max_memory_per_node }}"
        - name: CONFIG_QUERY_MAX_MEMORY
          value: "{{ .Values.trino.config.query_max_memory }}"
        - name: CONFIG_MEMORY_HEAP_HEADROOM_PER_NODE
          value: "{{ .Values.trino.config.memory_heap_headroom_per_node }}"
        - name: CONFIG_MAX_SPILL_PER_NODE
          value: "{{ .Values.trino.config.max_spill_per_node }}"
        - name: CONFIG_QUERY_MAX_SPILL_PER_NODE
          value: "{{ .Values.trino.config.query_max_spill_per_node }}"
        # Custom envvars for e.g. templated credentials in custom connector configs
        {{- range .Values.trino.custom_env }}
        - {{ . | toJson }}
        {{- end }}
        resources:
          limits:
            memory: {{ .Values.trino.mem_limit_worker }}i # "nG" => "nGi"
        ports:
        - name: http
          containerPort: 8080
        - name: hivecache-data
          containerPort: 8898
        - name: hivecache-bk
          containerPort: 8899
        # Custom ports for e.g. exposing additional hive caches
        {{- range $name, $port := .Values.trino.custom_ports }}
        - name: {{ $name }}
          containerPort: {{ $port }}
        {{- end }}
        volumeMounts:
        - name: trino-config
          mountPath: /tmp/roconf
        - name: trino-catalog
          mountPath: /tmp/rocatalog
        - name: trino-etc
          mountPath: /trino-server/etc
        - name: trino-memcache
          mountPath: /memcache
        - name: trino-cache
          mountPath: /diskcache
        - name: storage
          mountPath: /data
{{- if .Values.alluxio.enabled }}
        - name: alluxio-domain
          mountPath: /opt/domain

      - name: alluxio-worker
        image: {{ .Values.image.alluxio }}
        imagePullPolicy: IfNotPresent
        command: ["tini", "--", "/entrypoint.sh"]
        args:
        - worker-only
        - --no-format
        env:
        - name: ALLUXIO_WORKER_HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        - name: ALLUXIO_WORKER_CONTAINER_HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: ALLUXIO_MASTER_MOUNT_TABLE_ROOT_UFS
          value: {{ .Values.alluxio.root_mount }}
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: minio-root
              key: user
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: minio-root
              key: pass
        envFrom:
        - configMapRef:
            name: alluxio-config
        readinessProbe:
          tcpSocket:
            port: alluxio-rpc
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
          failureThreshold: 3
          successThreshold: 1
        livenessProbe:
          tcpSocket:
            port: alluxio-rpc
          initialDelaySeconds: 15
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 2
        ports:
        - name: alluxio-rpc
          containerPort: 29999
        - name: alluxio-web
          containerPort: 30000
        volumeMounts:
        - name: alluxio-domain
          mountPath: /opt/domain
        - name: alluxio-memcache
          mountPath: /dev/shm
        - name: alluxio-cache
          mountPath: /mnt/cache
{{- if and .Values.alluxio.nfs_server .Values.alluxio.nfs_path }}
        - name: nfs-data
          mountPath: /mnt/nfs
{{- end }}

      - name: alluxio-job
        image: {{ .Values.image.alluxio }}
        imagePullPolicy: IfNotPresent
        command: ["tini", "--", "/entrypoint.sh"]
        args:
        - job-worker
        env:
        - name: ALLUXIO_WORKER_HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        - name: ALLUXIO_WORKER_CONTAINER_HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: ALLUXIO_MASTER_MOUNT_TABLE_ROOT_UFS
          value: {{ .Values.alluxio.root_mount }}
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: minio-root
              key: user
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: minio-root
              key: pass
        envFrom:
        - configMapRef:
            name: alluxio-config
        readinessProbe:
          tcpSocket:
            port: job-rpc
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
          failureThreshold: 3
          successThreshold: 1
        livenessProbe:
          tcpSocket:
            port: job-rpc
          initialDelaySeconds: 15
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 2
        ports:
        - containerPort: 30001
          name: job-rpc
        - containerPort: 30002
          name: job-data
        - containerPort: 30003
          name: job-web
        volumeMounts:
        - name: alluxio-domain
          mountPath: /opt/domain
        - name: alluxio-memcache
          mountPath: /dev/shm
        - name: alluxio-cache
          mountPath: /mnt/cache
{{- if and .Values.alluxio.nfs_server .Values.alluxio.nfs_path }}
        - name: nfs-data
          mountPath: /mnt/nfs
{{- end }}
{{- end }} # alluxio.enabled

      volumes:
      - name: trino-config
        configMap:
          name: trino-config
      - name: trino-catalog
        configMap:
          name: trino-catalog
      - name: trino-etc
        emptyDir: {}
      - name: trino-memcache
        emptyDir:
          medium: "Memory"
          sizeLimit: {{ .Values.trino.mem_cache }}i # "nG" => "nGi"
{{- if .Values.alluxio.enabled }}
      - name: alluxio-domain
        emptyDir: {}
      - name: alluxio-memcache
        emptyDir:
          medium: "Memory"
          sizeLimit: {{ .Values.alluxio.mem_cache }}i # "nG" => "nGi"
{{- if and .Values.alluxio.nfs_server .Values.alluxio.nfs_path }}
      - name: nfs-data
        nfs:
          server: {{ .Values.alluxio.nfs_server }}
          path: {{ .Values.alluxio.nfs_path }}
{{- end }}
{{- end }}

  volumeClaimTemplates:
  - metadata:
      name: storage
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: {{ .Values.trino.disk_worker }}i # "nG" => "nGi"
  - metadata:
      name: trino-cache
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: {{ .Values.trino.disk_cache }}i # "nG" => "nGi"
{{- if .Values.alluxio.enabled }}
  - metadata:
      name: alluxio-cache
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: {{ .Values.alluxio.disk_cache }}i # "nG" => "nGi"
{{- end }}
